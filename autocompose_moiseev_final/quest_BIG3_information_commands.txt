+1)развернуть стек при помощи .yml
-2)придумать как сравнить 2 файла .yml 

не АВТОМАТИЗИРуют обычно эти КОМАНДЫ  -в БИГ3 инфра разворачивается отдельно
docker swarm init
docker swarm join --token SWMTKN-1-0dvj3objata2br4uzio7rxgfasvhpugh5fg7l9mc5arp32w101-3yu5qkv291cvmpvnmx588umg4 10.128.0.23:2377
docker node ls


docker stack deploy -c stack-test-mpa.yml webserver_test

docker logs webserver_test_webserver-manager



1)добавить в докер сварм сервисы
2)запустить docker service inspect --pretty frontend  и сравнить фактический конфиг с изначальным.


+1)создать виртуальный сервер -аналог rpn-dev(manager 185.175.47.191), версия докера 20.10.5	
+2)поставить докер-сварм, добавить на вирт.сервер стек  (docker swarm init)
+3)попытаться забрать данные о фактическом стеке различными способами, (например, по пути cd /var/lib/docker/swarm/worker/tasks.db)
-3.1)скачал tasks.db, как открыть?  (sqlite3 не открывает, boltdb не ставится, dbtoyaml неясно как пользовать)
-3.2) dbtoyaml выдает ошибку psycopg.OperationalError: connection failed: FATAL:  database "tasks.db" does not exist
+4)скачать с сервера нужные данные, команда 	
	scp root@185.175.47.191:/var/lib/docker/swarm/worker/tasks.db  /directory  --или /C:\

-5)попытаться сравнить фактические данные с исходными любыми способами

мой тестовый сервак YC analog-big3-rpn-dev, юзер moiseevpa
analog-big3-rpn-dev-manager
158.160.36.33
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQC4xIRU61jBvNU3OgRZX6FoWcV76P1ARH1pZFtojxPs6ge6peQkC9jrWPXa0+tiYmgrSe9FMabZjWwO3HEa5NY1g2YCLKNjHArBkFwN6yMWHIHL4X499dKF6Jqzj7bXcqQfQRmaEPnzdxMBzQN954kVN6GpYYc0/OCKaaR1L3h/MrcD+caioZFo0nWhM0udGOUkooBeRRYkWiyRyjTyLY3DgJAH2sGF2eX9jJkXqFcAMK+G9uD6uViGVvpfxLnhL4/K/BvhAf7eYz9WatIuP0DiHzVwNPQM0CNFpk7ILn4t7puCnjXf7HmxesYCF982Uj4NNCeN4CLipgBYf2CzEG6YX/w3Y6ctjDNiDzoYxfNNvMWlv/6wkaOf4+VhdfQcNq88xH5G4wRtSaZslTyuGmn3UFp1G048CDC+cLvCd+NNTOSzUu8Vea/83mdDgq8+VHdQOFUfSELjqtWas9vd+zemmh/d4MqaywgWfNr8XdIxVAX6yEVJKAMk4Tqlv0h/uwE=

analog-big3-rpn-dev-worker
84.201.157.213

-----------------------------------------------------------------
Данные VPN от  Евгений Улитин
впн l2tp+ipsec
server - ....
user -  ...
pass - xxxxxxxxxxxxxxxx
ipsec pre share key - xxxxxxxxxxx
-----------------------------------------------------------------

*****************************************************************
ЗАДАНИЕ Сергей Федосов.
https://git.big3.ru/rpn/services/main   Бэкенд
https://portainer.rpn.big3.ru/#/stacks
пароль  tre45tre

ПРОВЕРКА ВРЕМЕНИ ФАКТИЧЕСКОЙ СБОРКИ ОБРАЗОВ БЭКА, ФРОНТА
https://develop.v2.rpn.big3.ru/login          -логинимся
https://develop.v2.rpn.big3.ru/build-date.txt -версия фронта, дата.
https://develop.v2.rpn.big3.ru/api/version    -версия бэка


***
Сергей Федосов BIG3 devops, [28.03.2023 19:38]
И мысль такая, что доступ выдаст не для экспериментов, а если уже есть план что сделать нужно
ты бы действительно локально поэкспериментировал, нашел способ а там на сервере уже по готовому плану действовал
***


ЗАДАЧА
Без передеплоя меняли вручную- переменные окружения, КЕШ(с уже идемпотентными запросами GET к примеру).
Найти как выгрузить конфигурацию фактического стека?

1) отправить ssh ключ коллеге BIG3 Евгений Улитин
-Чтобы выгрузить конфиг нужен доступ к докеру. Консольно или апи.
-для выгрузки фактического стека
-для работы в консоли/api контейнера в портейнере 
(-чтобы на этот сервер по ssh зайти, а на нём посмотреть данные по пути cd /var/lib/docker  /swarm/worker   /tasks.db)



VPN
-доступ к VPN.big3 для работы с тестовым сервером develop.rpn.big3.ru  (admin@localhost  nimda)
...
каждый стек называется по имени ветки и деплоится также по имени ветки  
https://develop.v2.rpn.big3.ru/company
логин admin@localhost
пароль nimda


ЗАДАНИЕ
1)найти возможность выгрузить конфиги 
2)найти самый простой способ актуализировать ямль файл на основе новых изменений 
---
1)развернуть стек
2)загуглить как выгрузить конфигурацию    В ЯМЛ ФОРМАТЕ!!!  
3)на тестовом стенде смотреть как и что выгружается


докер сварм наверное хранит в ДЖЕЙСОНЕ в SQL базе   ФАЙЛ TASKS.DB
в крайнем случае выгрузка в том виде  в котором есть и сравним ее с конфигом свежого деплоя
Хранит здесь? /var/lib/docker/swarm/worker  ???   tasks.db
cd /var/lib/docker  /swarm/worker   /tasks.db


----------------------------------------------------------------------
ПОПРОБОВАТЬ ПОЭКСПЕРИМЕНТИРОВАТЬ
КУБЕР - малый репозиторий  с минимальной конфигурацию
например миникуб-локал.разработка - ранее была проблема с драйвером сети


+++++++++++++++++
в VARIABLES только подмена). Всё переменные в .env файлах прямо в репе
ПЕРЕМЕННЫЕ ХРАНЯТСЯ ЗДЕСЬ
https://git.big3.ru/rpn/services/main/-/blob/develop/app/.env.base


!!!!!!!!!!!!!!!!!!!
424 контейнера это все что на сервере работает. 
Надо только rpn-backend-testing
 https://portainer.rpn.big3.ru/#!/1/docker/stacks/rpn-backend-testing?type=1&external=true 
     (это один стэк testing.v2.rpn.big3.ru) например.

А yaml-файл его лежит вот тут РосПриродНадзор-Госуслуги-Основное приложение-Repository
 https://git.big3.ru/rpn/services/main/-/blob/develop/stack.yml
!!!!!!!!!!!!!!!!!!!












---
в kubernetis оказывается команда есть для выгрузки ямла)
kubectl get pod jump-pod -o yaml
kubectl get pod my-pod -o yaml                # Get a pod's YAML
---
+++++++++++++++++


















----------------------------------------------------------------------------
КОМАНДЫ ДЛЯ ВЫПОЛНЕНИЯ КВЕСТА 1 Сергей.Ф.

сменил пароли рут,юзера
sudo apt update
sudo apt install apt-transport-https ca-certificates curl software-properties-common
sudo apt install gpg
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb\_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
apt-cache policy docker-ce
sudo apt install docker-ce
sudo systemctl status docker
sudo usermod -aG docker moiseevpa
su - moiseevpa

1)На manager ноде создаем сварм инит
moiseevpa@analog-big3-rpn-dev:~$ docker swarm init
Swarm initialized: current node (pxs2bbs0n9e51kaf6mv7h6sjt) is now a manager.

2)На воркер ноде запускаем команду присоединения к manager
 docker swarm join --token SWMTKN-1-0dvj3objata2br4uzio7rxgfasvhpugh5fg7l9mc5arp32w101-3yu5qkv291cvmpvnmx588umg4 10.128.0.23:2377

3)проверяем на Менеджере!!
docker node ls

????  4)ставим реджестри на линукс-Менеджер
????  sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2
????  Эта команда скачает registry и запустит контейнер на порту 5000 (ключ -p 5000:5000). Данный контейнер будет запускаться автоматически при старте docker.

может взять образ такой? 
wget https://nodejs.org/dist/v18.15.0/node-v18.15.0.tar.gz
tar xvfz node-v18.15.0.tar.gz

ИЛИ этот с РЕДЖЕСТРИ открытый?!?!!?
docker pull ibmcom/node-jsmin-ppc64le    -нет этот огромный какой-то


ЭТО НЕ РАБОТАЕТ ТАК КАК ЗАКРЫТА РЕПА ВИДАТЬ
------------5) на Менеджере
------------docker pull docker-registry.ru:5000/ptm:stage;

docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG]
docker tag  node:8 158.160.36.33/ptm:stage
docker push 158.160.36.33/ptm:stage






/////////////
!!!ЭТО !НЕ! СРАБОТАЛО
wget  https://www.sqlite.org/2023/sqlite-autoconf-3410200.tar.gz
tar xvfz sqlite-autoconf-3410200.tar.gz
cd sqlite-autoconf-3410200
sudo apt-get install gcc
sudo ./configure --prefix=/usr/local
sudo apt update && sudo apt upgrade -y
sudo apt install -y make
make
make install
/////////////

/////////////
!!!ЭТО СРАБОТАЛО
sudo apt-get install sqlite3
/////////////

sudo su
cd /var/lib/docker/swarm/worker/
не сработает-  sqlite3
не сработает-  .open tasks.db

НАМ НЕ ПОДОЙДЕТ SQLITE3.  НУЖНА BOLTDB  (данные хранятся в парах «ключ-значение»)
sudo apt update;
sudo apt install golang-github-boltdb-bolt-dev

СТАВИМ GO
wget https://dl.google.com/go/go1.11.2.linux-amd64.tar.gz
sudo tar -C /usr/local -xzf go1.11.2.linux-amd64.tar.gz
export PATH=$PATH:/usr/local/go/bin
source ~/.profile
export PATH=$PATH:/usr/local/go/bin

ЛИБО пробовать такое sudo apt update && sudo apt install golang

ЭТО ТОЖЕ НИЧЕГО НЕ СРАБОТАЛО
export GOPATH=/home/user/go
  go install https://pkg.go.dev/go.etcd.io/bbolt@v1.3.7/cmd/bbolt

СТАВИМ Pyrseas  DBTOYAML
apt install  pip
pip install Pyrseas

sudo apt install python3-psycopg2 
pip install psycopg2-binary

ИЗ КОНСОЛИ WINDOWS ПОДКЛЮЧАЕМСЯ 
ssh root@158.160.36.33
	scp root@158.160.36.33:/var/lib/docker/swarm/worker/tasks.db  /directory 
	--или /C:\
+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!
ПРЕОБРАЗОВАЛ СВОЙ закрытый id_rsa КЛЮЧ при помощи PUTTYGEN  В NEW ФОРМАТ .PPK И ПОДКЛЮЧИЛОСЬ С ВИНДЫ!!
ssh -i C:/Users/peter/.ssh/id_rsa.ppk moiseevpa@158.160.36.33

скачали ВИНДОЙ файл напрямую+++
scp -i C:/Users/peter/.ssh/id_rsa.ppk  root@158.160.36.33:/var/lib/docker/swarm/worker/tasks.db C:/Users/peter
+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!+++!!!!

E:
cd E:\PROGRAM\PuTTY
pscp [имя пользователя]@[имя сервера/ip-адрес]:[путь к файлу] [путь к файлу]

++++скачали ВИНДОЙ файл напрямую+++
sudo su
nano /root/.ssh/authorized_keys 
И УДАЛЯЕМ ВСЁ ДО ЗНАКОВ SSH
scp root@158.160.36.33:/var/lib/docker/swarm/worker/tasks.db /directory
это сработало:  scp -i C:/Users/peter/.ssh/id_rsa.ppk  root@158.160.36.33:/var/lib/docker/swarm/worker/tasks.db C:/Users/peter
+++++


-----
ставим ПОСТГРЕСС
# Create the file repository configuration:
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'

# Import the repository signing key:
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -

# Update the package lists:
sudo apt-get update

# Install the latest version of PostgreSQL.
# If you want a specific version, use 'postgresql-12' or similar instead of 'postgresql':
sudo apt-get -y install postgresql
-----

dbtoyaml help



*********
OperationalError: FATAL: роль "root" не существует

Вы найдете эту ошибку в файле: /var/log/postgrsql/postgrsql-10-main.log (в системе Linux).

Решение может быть:

    1- Откройте терминал и введите => sudo su postgres
    2- Введите пароль для пользователя системы postgres (если необходимо)
    3- createuser root -s
    4- psql template1
    5- изменить роль root с паролем 'yourpassword';
    6- \q
    7- выход
    8- перезапуск службы odoo
*********



***
 но новые записи создаются там только тогда, когда планируются новые задачи,
 поэтому вы сможете увидеть гораздо больше полезных данных о том, 
 как часто сервисы перезапускаются swarm docker service ps <servicename>.
***



Список заданий Swarm
docker service ps
!!! "docker service inspect redis" ---КОМАНДА "service inspect" ПОЗВОЛЯЕТ СОБРАТЬ ДАННЫЕ О сервисах
docker service inspect --pretty frontend ---КРАСИВЫЙ ВЫВОД
УЗНАТЬ ЧИСЛО РЕПЛИК docker service inspect --format='{{.Spec.Mode.Replicated.Replicas}}' redis

СТАВИМ NOSQL mongodb
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10
sudo apt-get install -y mongodb
echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list
sudo apt-get update
sudo apt-get install mongodb-org
sudo systemctl unmask mongod
sudo service mongod start

mongo

use tasks    -переключаемся на нашу базу.
db.getName ()   -имя текущей базы
db.getCollectionNames ()   -отобразит коллекции

db.collectionName.findOne()




-----------------------------------------------------------------------
 docker registry. Swarm 
Шаг 1 — Установка и настройка реестра Docker

mkdir ~/docker-registry
cd ~/docker-registry
mkdir data
nano docker-compose.yml

version: '3'

services:
  registry:
    image: registry:2
    ports:
    - "5000:5000"
    environment:
      REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data
    volumes:
      - ./data:/data
	  
ДАлее
sudo apt install docker-compose	  
sudo service docker restart
docker-compose up

---1. Add user to docker group (if not already added)
---sudo usermod -aG docker $USER
---2. create a symbolic link to /usr/bin using the following command
---$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
+++3. Restart docker service
+++$ sudo service docker restart

---
ЛИБО ИСПОЛЬЗУЕМ ДАННУЮ КОМАНДУ для локального
docker run -d -p 5000:5000 --restart=always --name registry registry:2

docker ps  -просмотр запущенных контейнеров
docker stop 66bca71a9319
docker rm 66bca71a9319

-------------------------------
ПЫТАЕМСЯ ВСЕ ЖЕ СДЕЛАТЬ ЛОКАЛЬНОЕ ХРАНИЛИЩЕ
-------------------------------
docker run -d -p 5000:5000 --restart=always --name registry registry:2

docker tag microsevice_v1 158.160.36.33:5000/microsevice_v1
-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------



+++++ПРОБУЕМ СДЕЛАТЬ ТАК++++++++
теперь можно запустить веб-сервис в Docker Swarm Mode.
За счет следующей строки кода выполнится развертывание службы веб-сервера:
docker service create --name webserver -p 80:80 httpd
docker service ps webserver 

Запустим службу масштабирования веб-сервера 
docker service scale webserver=2
docker service ps webserver 

ЧИСТКА
docker service ls
docker service rm gy53sx5suq3w

ВЫВОД КОНФИГА ФАКТИЧЕСКОГО
docker service inspect --pretty webserver

docker service inspect webserver

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
скрипт написать такой
+1)на manager-ноде взять вывод "docker service ls"
+2)его отфильтровать скриптом - взять имена всех сервисов =  переменная $val123
3)далее запуск команды "docker service inspect --pretty $val123"
4)сохранение вывода фактического стека в файл.
5) забираем скриптом этот файл по ssh на свой рабочий ПК и сравниваем вручную или спецСкриптом с  исходным stack.yml


Сделаем файл исполняемым:
chmod +x ./myscript
Теперь попытаемся его выполнить:
./myscript

#!/bin/bash
# our comment is here
echo "The The TheTheTheTheThe"

---------------chmod +x ./script_stack_fact.sh     

 chmod +x moiseevpa_stack_script.py
 python3 moiseevpa_stack_script.py
 
!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!https://github.com/Red5d/docker-autocompose
!!!!!!!!!!!!!!!!!
!!!!!!!!!docker-autocompose!!!!!!!!создавать файлы docker-compose для управления контейнерами, которые я настроил вручную
При внесении изменений проверьте выходные данные скрипта, записав их в файл (docker-compose.yml или docker-compose.yaml) 
и запустив «docker-compose config» в той же папке, что и полученный результат. файл будет принят docker-compose.

 1) выгружаем  стек 1   и Стек 2
 2) СРАВНИВАТЬ 
 -перемен окруж(их название и их содержание),  
 -порты, 
 -лейблы,
 -сети,
 -валиумы,   
 -СКЕЙЛ(репликс), 
 -рестарт полиси,
 -апдейт конфигуреейщн
 -placement constraints
 
 КОМАНДА МОЖЕТ ВЫВЕСТИ ТОЛЬКО ДАННЫЕ С КОНТЕЙНЕРОВ НО НЕ ДАННЫЕ С СЕРВИСОВ SWARM
 !!!команда все данные выведет по стеку
 Чтобы распечатать все контейнеры в формате docker-compose:
!!!!!это КАК В README утилиты autocompose-  это мы смотрим все контейнеры запущенные
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/red5d/docker-autocompose $(docker ps -aq) | tee autocompose.yml

!!!!!а это мы смотрим все сервисы запущенные
!!!Эта команда выводит список служб, запущенных в рое.
!!!Это команда управления кластером, и она должна выполняться на узле диспетчера роя.
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/red5d/docker-autocompose $(docker service ls) | tee autocompose.yml
-не выполнилось так как  !!!  That container is not available.!!!-это ошибка которую выдает САМ код скрипта

-не помогло- ЗДЕСЬ УКАЗАЛ ТОЛЬКО ТЕ АЙДИШНИКИ КОТОРЫЕ НЕ ЯВЛЯЮТСЯ ПОРТЕЙНЕРом
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/red5d/docker-autocompose kp8bk2o3een5 2glvo0b2p6n1 | tee autocompose.yml


НАДО ПЕРЕДЕЛЫВАТЬ КОД autocompose под СЕРВИСЫ. (все реплики контейнеров одного сервиса одинаковы)
-переделал
-новая команда с моим гитом

!не работает
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock github.com/peter91peter91/big3_test_repository/tree/main/docker-autocompose-master $(docker service ls) | tee autocompose.yml
!не работает
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock https://github.com/peter91peter91/big3_test_repository $(docker service ls) | tee autocompose.yml
!не работает
docker run --rm -e /var/run/docker.sock:/var/run/docker.sock SITE_URL=https://github.com/peter91peter91/big3_test_repository/tree/main/docker-autocompose-master $(docker service ls) | tee autocompose.yml
!не работает
docker run --rm -e /var/run/docker.sock:/var/run/docker.sock SITE_URL=https://github.com/peter91peter91/big3_test_repository $(docker service ls) | tee autocompose.yml

не работает!
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock https://github.com/peter91peter91/docker-autocompose $(docker service ls) | tee autocompose.yml

docker run --rm -e /var/run/docker.sock:/var/run/docker.sock SITE_URL=https://github.com/peter91peter91/docker-autocompose  $(docker service ls) | tee autocompose.yml


ПРИЧИНА. НАДО СОЗДАТЬ УПАКОВКУ ОБРАЗА docker-autocompose -как?? .  pkgs/container 

СОЗДАЛ ОБРАЗ!!
 docker images
REPOSITORY                         TAG        IMAGE ID       CREATED          SIZE
<none>                             <none>     5c0b2ba23b78   22 minutes ago   51.8MB

МЕНЯЕМ ИМЯ если нету
docker tag 5c0b2ba23b78 autocompose:latest

ЗДЕСЬ ИМЯ АЙДИ ОБРАЗА
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock 5c0b2ba23b78 $(docker service ls) | tee autocompose.yml

docker run -t -d 5c0b2ba23b78      -тут подключён виртуальный терминал и bash не вылетит, а останется работать.

CONTAINER ID   IMAGE                        COMMAND                  CREATED         STATUS         PORTS                          NAMES
c9c9db0ee88c   5c0b2ba23b78                 "python3"                5 seconds ago   Up 2 seconds                                  crazy_northcutt


https://ghcr.io/Red5d/docker-autocompose
https://github.com/peter91peter91/docker-autocompose


ИНСПЕКТ НАДО ДЕЛАТЬ КОНТЕЙНЕРА А НЕ СЕРВИСА!!!!!!!!!
"при переезде серверов меняются только метки"
!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!




!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!портейнер разворачиваем!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
sh deploy.sh
разворачивает stack1.yml

если нет доступа по http://158.160.36.33:9000/

то в настройках суперпутти можем добавить -L 9000:9000

 сделали alias подключение к облачной.машине
analog-big3-rpn-dev-manager-alias
с extra putty arguments!!:     -L 9000:9000


http://158.160.36.33:9000/	
admin	Ab167321Ab167321

docker service scale portainer_portainer=0
sudo mc   -  удалить все файлы в вольюме портейнера   вар-либ-докер-вольюмез-портейнер-дата 
docker service scale portainer_portainer=1
 
если в портейнере down, то возможно бит файл tasks.db
 
docker swarm leave --force
swarm init
 
очистка docker prune    (volumes networks старые)
https://github.com/Red5d/docker-autocompose
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


/////////////
!!не так опять что то делаю. создал свой образ на основе переделанного скрипта docker-autocompose
docker run -t -d 5c0b2ba23b78   -так он запускается.
!!а так нет  docker run --rm -v /var/run/docker.sock:/var/run/docker.sock c9c9db0ee88c $(docker service ls) | tee autocompose.yml
-ты ему передаешь в аргументах команду для запуска, в контейнере её нет получается
/////////////


**********************************************************
записали файлы в папку
/docker-autocompose-master$ ls
 autocompose.py   Dockerfile   README.md   requirements.txt   setup.py  'Source code.zip'


забилдили свой образ 
 docker build /home/moiseevpa/docker-autocompose-master/

но не работает
  docker run --rm -v /var/run/docker.sock:/var/run/docker.sock c9c9db0ee88c $(docker service ls) | tee autocompose.yml
**********************************************************


pip install pyyaml  pip install pyaml- НЕ СРАБОТАЛО

!!!!!!!!!!БЕЗ ЭТОГО наша утилита  НЕ ПОНИМАЕТ МОДУЛЬ pyaml!!!!!!!
+++++  sudo easy_install pip
+++++  sudo python3 -m pip install pyaml

далее обязательно!!!! python3  
cd /home/moiseevpa/docker-autocompose-master/
sudo python3 autocompose.py  gwd2huglp63a


  File "/home/moiseevpa/docker-autocompose-master/autocompose.py", line 147, in generate
    cid = [x.short_id for x in c.services.list(all=True) if cname == x.name or x.short_id in cname][0]

__________________________________________________________________________________________________________
27.04.2023 -планы-
1)ПЕРЕДАВАТЬ В СКРИПТ АЙДИ В РЕЖИМЕ ОТЛАДКИ   (-сложно сделать так как подтягивается из окружения)
И СМОТРЕТЬ ПОЧЕМУ ОН НЕ ЕСТ
       cid = [x.short_id for x in c.services.list(all=True) if cname == x.name or x.short_id in cname][0]
	   list(all=True)  Возвращает значение true, если все выражения в списке list равны true.
2)пробовать делать package
__________________________________________________________________________________________________________



!!!отладка
1)ЗАХОДИМ "python console"
2)вводим там import docker  -  импортируем докер библиотеку в "python console"   
3) запускаем     c = docker.from_env()  



print(dir(docker))

все команды в консоли питона 

import  docker
client = docker.APIClient(base_url='unix://var/run/docker.sock')
client.version()
c = docker.from_env()  
.env
		

docker.errors.DockerException: Error while fetching server API version: (2, 'CreateFile', 'Не удается найти указанный файл.')
-   не может добавить _url
-   Выяснить где и какая base_url

Запрос  __doc__  покажет какие методы доступны
https://www.geeksforgeeks.org/code-introspection-in-python/
__doc__	Return some documentation about an object


1)проверили что подключен плагин docker

1)вручную поставил  Successfully installed pip-23.1.2
C:\Users\peter\AppData\Local\Programs\Python\Python311\python.exe -m pip install --upgrade pip

ПЕРЕСТАВИЛ ПИТОН ,ПОСТАВИВ ГАЛКУ УСТАНОВИТЬ PIP

2)ВРУЧНУЮ В ВИНДЕ ПРОПИСАЛ В PATH 
адрес до PIP  -ЧТОБЫ ТАМ ИСКАЛСЯ PIP

3)ДАЛЕЕ СКАЧАЛ ПАЙТОН 3.8 ЧТОБЫ ИСПРАВИТЬ ОШИБКУ ОТСУТСТВИЯ ДИСТРИБУТИВОВ ДЛЯ ДОКЕР 4.4
!!! +++  все нужные версии поставились requirements +++ !!!


!!!
Как добавить в PATH Python
Чтобы добавить директорию Python к пути для определенного сеанса в Unix:
В командной оболочке csh введите setenv PATH «$PATH:/usr/local/bin/python» и нажмите Enter.
В командной оболочке bash (Linux) введите export PATH=«$PATH:/usr/local/bin/python» и нажмите Enter.
!!!


ЭТО ЕСЛИ ПЫТАТЬСЯ ВСЁ ДЕЛАТЬ НА ПОСЛЕДНЕЙ ВЕРСИИ ПАЙТОН
!!!!!!ОШИБКА УСТАНОВКИ ДОКЕРА ИЗ RequirementS!!!!!!!!!!!!!!!!
Collecting docker==4.4.4
  Using cached docker-4.4.4-py2.py3-none-any.whl (147 kB)
Requirement already satisfied: six>=1.4.0 in c:\users\peter\appdata\local\programs\python\python311\lib\site-packages (from docker==4.4.4) (1.16.0)
Requirement already satisfied: websocket-client>=0.32.0 in c:\users\peter\appdata\local\programs\python\python311\lib\site-packages (from docker==4.4.4) (1.5.1)
Requirement already satisfied: requests!=2.18.0,>=2.14.2 in c:\users\peter\appdata\local\programs\python\python311\lib\site-packages (from docker==4.4.4) (2.28.2)
INFO: pip is looking at multiple versions of docker to determine which version is compatible with other requirements. This could take a while.

ERROR: Could not find a version that satisfies the requirement pywin32==227; sys_platform == "win32" (from docker) (from versions: 303, 304, 305, 306)
ERROR: No matching distribution found for pywin32==227; sys_platform == "win32"







+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
ПОСТАВИЛ  WSL . ПОСЛЕ RESTART ПК:

Установка Ubuntu уже выполнена.
Запуск Ubuntu...
Installing, this may take a few minutes...
Please create a default UNIX user account. The username does not need to match your Windows username.
For more information visit: https://aka.ms/wslusers
Enter new UNIX username:
moiseevpa
password ...

moiseevpa@DESKTOP-3Q8BEP7:~$
 sudo apt update
sudo apt install python3-pip
 sudo pip3 install virtualenv 
  python3 -m virtualenv pythonenv
 !!! переходим в наше вирт окружение
  source pythonenv/bin/activate
   pip install django
   
   затем пытался поставить ПЛАГИН WSL Support Framework  ЧТОБЫ INTERPRETER УВИДЕЛ WSL !!! НО !!!
   Not compatible with the version of your running IDE (PyCharm 2023.1)
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


cattrs  - это  все атрибуты сервиса с выбранным айди
    cattrs = c.services.get(cid).attrs
сделал вывод cattrs для модуля gwd2huglp63a   .   Его имя- это у нас  portainer_agent




+++++++КОНТЕЙНЕР!!!+++++++++++autocompose_original ++++++++++++++++++
 git clone https://github.com/peter91peter91/autocompose_original
 cd ~/autocompose_original  
 или cd /home/moiseevpa/autocompose_original  

git pull --ff-only        или git pull

docker ps
 ----> ---->  ---->   webserver_test_webserver-worker.1.b0v84nn80wyhlns84fzfgubbd
 sudo python3 autocompose.py  b28e96469c2d
 или
 python3 /home/moiseevpa/autocompose_original/autocompose.py  b28e96469c2d
 
 !!!ДАЛЕЕ НАЧИНАЕМ СРАВНИВАТЬ ВЫВОД С   autocompose.yml   !!!!!
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/red5d/docker-autocompose $(docker ps -aq) | tee autocompose.yml
---эта команда мусорит  в docker -ps -a  !!!
ПРОБУЕМ ТАКУЮ!!!
 python3 /home/moiseevpa/autocompose_original/autocompose.py $(docker ps -aq) | tee autocompose.yml     
 ПРОБУЕМ ТАКУЮ!!!
 ls -l -c     ---покажет время изменения файла
 nano autocompose.yml
 
 git config --global user.name "Petr Moiseev";
git config --global user.email peter__91@mail.ru;
git commit --amend --reset-author
 ??  создаем новую ветку или работаем в мастере?
 ??  git checkout -b  cloud-testapp
 комиттим
 git commit --amend -m "сохраним в вывод autocompose.yml v3  "
 git push --set-upstream origin master
 
 Support for password authentication was removed on August 13, 2021.
ДЕЛАЕМ ТОКЕН  -гитхаб-сеттингс.  Generate a personal access token f
вводим название   dev_manager_token

включаем отслеживание!!!!!  если файл какой создаем 
git add autocompose.yml
 git commit --amend -m "сохраним в вывод autocompose.yml v.2 "
 
ДАЛЕЕ ДЕЛАЕМ ПУШ
peter91peter91
ПАРОЛЬ 
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
git pull
  git push --set-upstream origin master
++++++++++++++++++autocompose_original ++++++++++++++++++





++++++СЕРВИС!!!++++++++++++autocompose_service ++++++++++++++++++
 git clone https://github.com/peter91peter91/autocompose_service
 cd ~/autocompose_service
 или cd /home/moiseevpa/autocompose_service/docker-autocompose-master/

git pull --ff-only        или git pull

 docker service ls
 ----> ---->  ---->    webserver_test_webserver-worker
sudo python3 autocompose.py   2glvo0b2p6n1
или
 python3 /home/moiseevpa/autocompose_service/docker-autocompose-master/autocompose.py  2glvo0b2p6n1
++++++++++++++++++autocompose_service ++++++++++++++++++

удаление НЕПУСТОЙ директории!!!!!! sudo rm -rf docker-autocompose-master 



///////////////////////////////////////////////////////
Сергей Федосов BIG3 devops, [16.05.2023 20:04]
под линукс все делается просто.
1. устанавливается докер + docker swarm init
2. клонируется репозиторий
3. переходим в папку dev склонированного репа и запускаем sh stack.sh

готово. После того как деплой пройдет, локальная копия должна заработать

а ну да... cp app/.env.local.example app/.env

https://git.big3.ru/rpn/services/main/-/blob/develop/INSTALL.md 
подробная инструкция
///////////////////////////////////////////////////////

---------------------------------------------
---------------------------------------------
---------------------------------------------
22.05.2023
 возьми готовый yaml стэка,
 задеплой и выгрузи обратно, сравни их

запускаем такой командой ОРигинал автоКомпоуза 
python3 autocompose.py   $(docker ps -aq) | tee autocompose.yml
---------------------------------------------
---------------------------------------------
---------------------------------------------
https://git.big3.ru/rpn/services/main/-/blob/develop/dev/stack.yml
https://git.big3.ru/rpn/services/main/-/blob/develop/dev/stack.sh



!!!!!!!!!!!!!!!!!!!!!25.05.2023!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
работает вывод
    print(cattrs.get("Spec", {}).get("TaskTemplate", {}).get("Placement", {}).get("Constraints", {}))
ТЕПЕРЬ ПРОБУЮ СДЕЛАТЬ
1)написать и ВЫЗЫВАТЬ ФУНКЦИЮ ИЗВЛЕЧЕНИЯ ПЛЕЙСМЕНТ 
2)И ВСТАВКИ ЕЕ В ОБЩИЙ МАССИВ VALUE
3)берем имя контейнера и ищем его в сервисах и далее подставляем в 
cattrs = c.services.get(cid).attrs
**********************
26,05,2023
искать айди можно по полю "ID":"chd7uh460n0t(здесь надо отрезать)lp6hfau3bmrn9"
вывести пары всех сервисов 
"ID":"chd7uh460n0t|||lp6hfau3bmrn9" - "Name":"rpn-backend-develop_cron"
**********************
или пары имя- айди
c.services.get(cid).name
rpn-backend-develop_cron
"Name":"rpn-backend-develop_cron"   ---    <Service":"chd7uh460n0t>
**********************
или просто 
1)берем все имена по списку айди c.services.list()
2)вывод c.services.get(cid).name ===  rpn-backend-develop_cron
3)а потом  ищем наш айди по по имени нужного сервиса!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


**************************************************************
**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************
**************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

how do I add a service to the swarm stack without overloading?
ответ GPT4

To add a service to the swarm stack without overloading, follow these steps:

1. Assess the current state of the swarm stack: Check the existing services, their configurations, and resources utilized by the stack. This will help you determine whether new services can be added without causing an overload.

2. Design the new service: Plan the new service by defining its purpose, configuration, and required resources. Be cautious not to exceed the capacity of the stack, especially in terms of CPU, memory, and storage.

3. Test the new service: Before adding the service to the swarm stack, test it in an isolated environment to ensure its stability and correctness. This will help you identify any performance bottlenecks or resource constraints that might cause overloading.

4. Update the stack definition file: Once the new service is ready, update the stack definition file (typically a 'docker-compose.yml' file). Add the new service specification, making sure it's compatible with the existing services and their configurations.

5. Deploy the updated stack: After updating the stack definition file, use the following command to deploy the updated stack:

```
docker stack deploy -c docker-compose.yml <stack-name>
```

Replace `<stack-name>` with the name of your stack.

6. Monitor the stack: After deploying the updated stack, monitor the swarm cluster and services for any signs of overloading. Keep an eye on CPU, memory, and storage usage. If you notice any performance issues, you might need to optimize the services, adjust resource allocation, or scale the swarm cluster by adding more nodes to distribute the workload.

7. Scale the services if necessary: If you find that the new service is causing an overload, you can scale the services horizontally by increasing the number of replicas for each service. This can be done using the following command:

```
docker service scale <service-name>=<number-of-replicas>
```

Replace `<-name>` with the name of your service and `<number-of-replicas>` with the desired number of replicas.

Remember that adding a service to a swarm stack without overloading requires careful planning and monitoring to ensure the stack remains stable and operates efficiently.

**********************************************************************************************************
**********************************************************************************************************
**********************************************************************************************************

как мне добавить службу в стек swarm без перегрузки?
ответ GPT4

Чтобы добавить службу в стек swarm без перегрузки, выполните следующие действия:

1. Оцените текущее состояние стека swarm: проверьте существующие службы, их конфигурации и ресурсы, используемые стеком. 
Это поможет вам определить, можно ли добавлять новые службы, не вызывая перегрузки.

2. Разработайте новую службу: Спланируйте новую службу, определив ее назначение, конфигурацию и требуемые ресурсы. 
Будьте осторожны, чтобы не превышать емкость стека, особенно с точки зрения процессора, памяти и хранилища.

3. Протестируйте новую службу: Перед добавлением службы в стек swarm протестируйте ее в изолированной среде, 
чтобы убедиться в ее стабильности и корректности. Это поможет вам выявить любые узкие места в производительности
 или ограничения ресурсов, которые могут привести к перегрузке.

4. Обновите файл определения стека: Как только новая служба будет готова, обновите файл определения стека 
(обычно это docker-compose.файл yml'). Добавьте новую спецификацию сервиса, убедившись, что она совместима 
с существующими сервисами и их конфигурациями.

5. Разверните обновленный стек: После обновления файла определения стека используйте следующую команду для 
развертывания обновленного стека:

```
развертывание стека docker -c docker-compose.yml <имя стека>
```

Замените `<stack-name>` на имя вашего стека.

6. Мониторинг стека: После развертывания обновленного стека проверьте кластер swarm и службы на наличие любых признаков перегрузки. 
Следите за использованием процессора, памяти и хранилища. Если вы заметили какие-либо проблемы с производительностью, возможно, 
вам потребуется оптимизировать службы, скорректировать распределение ресурсов или масштабировать кластер swarm, добавив больше 
узлов для распределения рабочей нагрузки.

7. При необходимости масштабируйте службы: Если вы обнаружите, что новая служба вызывает перегрузку, вы можете масштабировать
службы по горизонтали, увеличив количество реплик для каждой службы. Это можно сделать с помощью следующей команды:

```
масштаб службы docker <имя службы>=<количество реплик>
```

Замените `<-name>` на название вашей службы и `<количество реплик>` на желаемое количество реплик.

Помните, что добавление службы в стек swarm без перегрузки требует тщательного планирования и мониторинга, 
чтобы гарантировать, что стек остается стабильным и работает эффективно.







































///////////////////////////////////
////////////разные данные пароли установки//////////////////////
////////////////////////////////////
////////////////////////////////////
https://git.big3.ru/rpn/services/main   Бэкенд


https://portainer.rpn.big3.ru/#/stacks
https://portainer.rpn.big3.ru/#!/1/docker/stacks

admin | tre45tre45
пароль tre45tre



Без передеплоя меняли вручную- переменные окружения, КЕШ(с уже идемпотентными запросами GET к примеру).
Найти как выгрузить конфигурацию фактического стека ?


это для КОНСОЛИ контейнера в портейнере - доступ к VPN.big3 для работы с тестовым сервером develop.rpn.big3.ru  
...


каждый стек называется по имени ветки и деплоится также по имени ветки  - 





ПРОВЕРКА ВРЕМЕНИ ФАКТИЧЕСКОЙ СБОРКИ ОБРАЗОВ БЭКА,ФРОНТА
https://develop.v2.rpn.big3.ru/login
https://develop.v2.rpn.big3.ru/build-date.txt - версия фронта, дата.
https://develop.v2.rpn.big3.ru/api/version - версия бэка



ЗАДАНИЕ
найти возможность выгрузить конфиги 
НАЙТИ самый простой спосбоб актуализировать ямль файл на основе новых изменений 

ЗАДАНИЕ
развернуть стек
загуглить как выгрузить конфигурацию    В ЯМЛ ФОРМАТЕ!!!  
на тестовом стенде смотреть как и что выгружается


докер сварм наверное хранит в ДЖЕЙСОНЕ в SQL базе   ФАЙЛ TASKS.DB
в крайнем случае выгрузка в том виде  в котором есть и сравним ее с конфигом свежого деплоя






КУБЕР - малый репозиторий  с минимальной конфигурацию
например миникуб-локал.разработка - ранее была проблема с драйвером сети


GIT  big3
мой пароль  ........
мой логин ...
git clone https://git.big3.ru/rpn/services/main.git
cd ~/big3/main/dev

Далее по инструкции всё делаем ,включая авторизацию  в реестре https://git.big3.ru/rpn/services/main/-/blob/develop/INSTALL.md
sh stack.sh    -запускаем поднимаем весь стек

удалить все сервисы поднятые   в sh stack.sh 
docker stack rm rpn-backend-develop


в портейнере сделал юзера 
admin
adminadminadmin  пароль

Федосов дал папку VENDOR - была ошибка  нехватки файлов  
Failed opening required '/app/vendor/autoload.php
распакуй в app чтобы получилось app/vendor


cd ~/big3/main/dev
bash stack.sh
sudo netstat -tulpn | grep :80

https://wiki.yandex.ru/platform/useful-links/grid-2022-09-06t184028/
пароли биг3

cp app/.env.local.example app/.env
это пока не понял зачем команда,не делал
////////////////////////////////////
////////////////////////////////////
////////////////////////////////////
////////////////////////////////////
////////////////////////////////////

++++++СЕРВИС!!!+++++который я залил себе в гит+++++++autocompose_service ++++++++++++++++++
 git clone https://github.com/peter91peter91/autocompose_service
 заходим в docker-autocompose-master

 git clone  https://github.com/peter91peter91/autocompose_original

git pull --ff-only        или git pull

 docker service ls
 ----> ---->  ---->    webserver_test_webserver-worker
sudo python3 autocompose.py   2glvo0b2p6n1
НЕКОТОРЫЕ ПАКЕТЫ НЕДОСТУПНЫ /ДОСТУПНЫ АДМИНУ python3 autocompose.py 
++++++++++++++++++autocompose_service ++++++++++++++++++

удаление НЕПУСТОЙ директории!!!!!! sudo rm -rf docker-autocompose-master

запускаем такой командой ОРигинал автоКомпоуза 
python3 autocompose.py   $(docker ps -aq) | tee autocompose.yml 

отдельно для модуля CRON ,  к примеру
python3 autocompose.py 569e90340b50| tee autocompose.yml
////////////////////////////////////
////////////////////////////////////
////////////////////////////////////



пробрасываем ssh ключ для ГИТХАБ

docker swarm leave
docker swarm init
docker login registry.rpn.big3.ru
"..............." registry.rpn.big3.ru

cd ~/big3/main/portainer
bash portainer.sh
ждем 1-10 минут

удалить все сервисы поднятые   в sh stack.sh 
docker stack rm rpn-backend-develop


cd ~/big3/autocompose_service/docker-autocompose-master
python3 autocompose.py chd7uh460n0t | tee autocompose.yml

cd ~/big3/autocompose_original
python3 autocompose.py  8a070581b0ed | tee autocompose.yml
python3 autocompose.py $(docker ps -aq) | tee autocompose.yml

вывод старой ориг.проги PRINT
cd ~/PycharmProjects/autocompose_original;
python3  autocompose.py $(docker ps -aq) >  autocompose.yml

вывод новой проги моей
cd ~/big3/autocompose_containers_with_services_constraints;
python3  autocompose_containers_with_services_constraints.py $(docker ps -aq) >  autocompose.yml

очистка ненужных сервисов и контейнеров
docker service rm  gdqj7fu3f66f 
docker rm $(docker ps -aq)

деплой
cd ~/big3/main/dev;
bash stack.sh


////////////////////////////////////
////////////////////////////////////
////////////////////////////////////

Привет!  Прога моя сейчас деплоит DEV стек без ручных вмешателств, все сервисы в UP.
Старался сильно структуру исходного кода не менять- вызываю самописные методы там,где необходимо.

Из жёстких костылей:  
1)сети в блоки объявления: проставляю  параметр сети "external": False, в сетях со словом internal  (ориг код проставляет True и не деплоится)
2)сети в блоки объявления: фильтруются на наличие слова internal,
3)директивы сетей в каждом модуле фильтруются на наличие слова internal и выводятся ,если только  в имени сети таковое имеется

Не костыли:
4) Не выводим сервисы по нахождению в их имени слова Portainer
5) не выводим "ipc" , т.к. swarm при деплое пишет, что не поддерживает
6) не выводим   "container_name" 

7)меняем имя сервисов на короткие  ,напр.  postgres-smev  -(иначе ошибка имени при деплое)
8)программа выводит данные по контейнерам в .yml файл
9)программа добавляет в блоки сервисов директиву  constraints, обращаясь в SERVICES(где происходит обработка алгоритмом поиска  сервиса соответствующего каждому контейнеру)


////////////////////////////////////
////////////////////////////////////
////////////////////////////////////


план от 09.06.2023
!!!блок networks ДОРАБОТАТЬ для деплоя на более чем 1 машине (с этим пока всё пытаюсь разобраться)

+1)включаем на яндексе manager worker ноды
+2)Помещаем на manager Папку DEV и запускаем наш стек,
-3)cd ~/big3/main/dev    - ДАЛЕЕ настраиваем всё(инстр.выше писал)
+4) подменяем  в одном из сервисов constraints  == worker   (подменил в почтовом сервисе)
+5) деплоим

-ошибка,обризаются айдишники на 2 символа

-6) считываем моей прогой конфиг и сравниваем, что не так
-7) дорабатываем код - скорее всего СЕТИ!
-8) Деплоим считанный доработанный конфиг
